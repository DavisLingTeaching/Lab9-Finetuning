{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-nn-1G02YK3"
      },
      "source": [
        "# Lab 9: Finetuning BERT-based models\n",
        "\n",
        "In this lab, we will explore finetuning a BERT model for classification. Your task will be to classify movie reviews as positive or negative (i.e. the task from HW2). Supporting code will help you with the actual training. Your job is to focus on formatting the input and evaluating the accuracy of your final model.\n",
        "\n",
        "Once you finish working on this lab, please download it as a .ipynb notebook and submit the notebook to Moodle.\n",
        "\n",
        "#### **What should I do if I run out of RAM?**\n",
        "\n",
        "The free GPUs that Colab assigns might not always reliable. Sometimes you code will run without issues, and other times you might run into RAM errors. For this reason, try to train your models on as much data as possible, but do not worry if you are not able to train it on all of the data. You can also try to run the models on your personal computers without using GPUs!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0U2TkED2XFD"
      },
      "source": [
        "\n",
        "### Guiding Questions\n",
        "\n",
        "1. How do we use pretrained transformer models?\n",
        "1. How do we finetune a neural model of language for classification?\n",
        "\n",
        "### Learning Objectives.\n",
        "\n",
        "1. Understand how to map classification to the context of transformers\n",
        "1. Hands on experience using pretrained models\n",
        "1. Build a classifer on top of a pretrained model\n",
        "1. Reason about your model and its abilities\n",
        "\n",
        "### Rubric\n",
        "\n",
        "| Question | Points |\n",
        "| ------| ----- |\n",
        "| load_data | 25 Points |\n",
        "| Reflection | 75 Points |\n",
        "\n",
        "### Deadline:\n",
        "\n",
        "November 12, 11PM EST\n",
        "\n",
        "### Submission format:\n",
        "ipynb file saved after running your code cells and submitted to Moodle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U33GNX-4AyWV"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ9NSOjHAyWW"
      },
      "source": [
        "We will use HuggingFace throughout this lab. Finetuning involves XX steps:\n",
        "\n",
        "1. Loading a pretrained model\n",
        "2. Loading our finetuning data\n",
        "3. Searching for good hyperparameters for our model\n",
        "4. Training our model\n",
        "5. Testing our model\n",
        "6. Saving our model\n",
        "\n",
        "HuggingFace comes with tools for doing all of these. I will give very brief overviews of them below and point you to their code base. This will be useful for at least some of your final projects, so please look over these materials on your own.\n",
        "\n",
        "### The model and the tokenizer\n",
        "\n",
        "HuggingFace hosts a large number of pretrained models. You can find them [here](https://huggingface.co/). The main library for working with these models is transformers (documentation [link](https://huggingface.co/docs/transformers/index)). Each model comes with a tokenizer which maps from words to ids for the relevant model.\n",
        "\n",
        "### The data format\n",
        "\n",
        "In order to finetune our model with HuggingFace we need our data formatted in a particular way. We will use their dataset library (documentation [link](https://huggingface.co/docs/datasets/index)). Consider the following (modified) sample from the movie review dataset we are using:\n",
        "\n",
        "    [{'text': 'Note that I did not say that it is',\n",
        "        'label': 0},\n",
        "    {'text': 'In what is arguably the best outdoor adventure film of all time,\n",
        "        four city guys confront nature\\'s wrath, in a story of survival.'  \n",
        "        'label': 1}]\n",
        "\n",
        "Notice that it is a list of dictionaries mapping text to their labels. You will write a data loader that does this step.\n",
        "\n",
        "### Hyperparameter search with Trainer\n",
        "\n",
        "Recalling, HW2 one thing we have to do is find good hyperparameters. Luckily, there exists libraries that facilitate this. We will use [optuna](https://optuna.org/) coupled with HuggingFace's libraries to find optimal hyperparameters for our model.\n",
        "\n",
        "### Training with Trainer\n",
        "\n",
        "To finetune our model, we need a way of training the model. HuggingFace has a utility called [Trainer](https://huggingface.co/docs/evaluate/main/en/transformers_integrations#trainer) that will handle this for us.\n",
        "\n",
        "### Testing with Evaluate\n",
        "\n",
        "Finally, we need to evaluate our model to see if it is any good. You've already done your own accuracy, precision, recall, and F1-scores before. Here will make use of HuggingFace's [Evaluate](https://huggingface.co/docs/evaluate/main/en/index) library to do the hard things for us.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KVFIsJCAyWW"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ivsLMhkAyWW"
      },
      "outputs": [],
      "source": [
        "!pip install optuna transformers datasets accelerate evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enEQBIsfAyWW"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import torch, evaluate, accelerate\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import glob\n",
        "import numpy as np\n",
        "from datasets import Dataset, load_dataset\n",
        "import optuna\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QaKuu8QAyWX"
      },
      "outputs": [],
      "source": [
        "# Set device depending on whether or not you have access to GPUs\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0k3M3VMTAyWX"
      },
      "source": [
        "## Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orqPhWm8AyWX"
      },
      "outputs": [],
      "source": [
        "modelname = \"distilbert-base-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(modelname, use_fast=True)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(modelname,\n",
        "                                                            num_labels=2).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfH-_W9-AyWX"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 18iyGEGz4csxVDUee5gqUafbmvovUBzvr\n",
        "!unzip -o sentiment_data.zip\n",
        "!rm -rf __MACOSX/"
      ],
      "metadata": {
        "id": "Foqv8n-gM8_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8jBpehuAyWY"
      },
      "outputs": [],
      "source": [
        "#TODO: 25pts\n",
        "def load_data(path: str, label2id: dict) -> list[dict]:\n",
        "    \"\"\" Loads movie review data into a dictionary\n",
        "    Args:\n",
        "        path (str): Path to sentiment data directory\n",
        "                    (e.g., sentiment_data)\n",
        "        label2id (dict): Dict mapping label (i.e. pos, neg)\n",
        "                        to numbers (e.g., {'pos': 0, 'neg': 1})\n",
        "    Returns:\n",
        "        data (list[dict]): List of dictionaries, see the markdown block above\n",
        "                            this cell.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doIXxb6hAyWY"
      },
      "outputs": [],
      "source": [
        "def getDataset(path: str, label2id: dict,\n",
        "               tokenizer:AutoTokenizer=None,\n",
        "              tokenize:bool=True,\n",
        "               percent:float = 0.25) -> Dataset:\n",
        "    \"\"\" Return HuggingFace Dataset instance\n",
        "    Args:\n",
        "        path (str): path to directory\n",
        "        label2id (dict): Dictionary mapping classification labels to id\n",
        "        tokenizer (AutoTokenizer): A HuggingFace pre-trained tokenizer\n",
        "        tokenize (bool): Whether to tokenize data. Default True\n",
        "    Returns:\n",
        "        (Dataset): HuggingFace Dataset instance\n",
        "    \"\"\"\n",
        "    data = load_data(path, label2id)\n",
        "    # Shuffle the data\n",
        "    random.shuffle(data)\n",
        "    data = data[:int(len(data)*percent)]\n",
        "    data = Dataset.from_list(data)\n",
        "    # Tokenize\n",
        "    if tokenize:\n",
        "        if tokenizer is None:\n",
        "            print('Pass a tokenizer')\n",
        "            return\n",
        "        data = data.map(lambda examples: tokenizer(examples[\"text\"],\n",
        "                                                   return_tensors=\"pt\",\n",
        "                                                   padding=True, truncation=True),\n",
        "                        batched=True).with_format(\"torch\")\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-zWhm8_AyWY"
      },
      "outputs": [],
      "source": [
        "train_small_dataset = getDataset(\"sentiment_data/train\", {\"pos\": 0, \"neg\": 1}, tokenizer, percent=0.05)\n",
        "train_dataset = getDataset(\"sentiment_data/train\", {\"pos\": 0, \"neg\": 1}, tokenizer, percent=0.25)\n",
        "eval_dataset = getDataset(\"sentiment_data/eval\", {\"pos\": 0, \"neg\": 1}, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sohXRw4NAyWY"
      },
      "source": [
        "## Hyperparameter search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeUXJVHIAyWY"
      },
      "source": [
        "Recall from HW2 that models involve hyperparameters. We often want to find optimal hyperparameters that will result in better models. We can automate this process using HuggingFace. What will need to do is install a hyperparameter optimization library. We will use [optuna](https://optuna.org/).\n",
        "\n",
        "At its core, hyperparameter optimization is about trying different configurations and comparing model performance. To facilitate this we will need a way of reseting our model so we can try new hyperparameters. The model_init function below does just that. Additionally, we will want to sample a smaller amount of data, since tuning can take a long time! The code below creates a set up that does this. Look over the code and make sure you understand its aims!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RD0ux0L3AyWY"
      },
      "outputs": [],
      "source": [
        "def model_init():\n",
        "    return AutoModelForSequenceClassification.from_pretrained(modelname, num_labels=2).to(device)\n",
        "\n",
        "# Uses accuracy is the metric at eval steps\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "# Set some initial parameters\n",
        "batch_size=20\n",
        "args = TrainingArguments(\n",
        "        f\"{modelname}-finetuned-movie-reviews\",\n",
        "        evaluation_strategy = \"epoch\",\n",
        "        save_strategy = \"epoch\",\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        num_train_epochs=2,\n",
        "        weight_decay=0.01\n",
        ")\n",
        "\n",
        "# Set up a trainer with less data\n",
        "trainer = Trainer(\n",
        "    model_init=model_init,\n",
        "    args=args,\n",
        "    train_dataset=train_small_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Find the best hyperparameters over 10 runs\n",
        "best_run = trainer.hyperparameter_search(n_trials=2, direction=\"maximize\",\n",
        "                                        backend=\"optuna\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MYvT5raAyWZ"
      },
      "source": [
        "## Train\n",
        "\n",
        "Now that we have some (hopefully) good hyperparameters, let's train our model on our full training data! This may take some time, so look over the reflection questions in the meantime!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAtDWIAqAyWZ"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(modelname,\n",
        "                                                            num_labels=2).to(device)\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "for n, v in best_run.hyperparameters.items():\n",
        "    setattr(trainer.args, n, v)\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPoCrMCHAyWZ"
      },
      "source": [
        "## Test\n",
        "\n",
        "Now that we've trained our model, we need to see if it's any good. Let's evaluate it on test data. First we load that data, then we make use of HuggingFace's evaluate library. We are interested in text classification here, so we use that task. In particular, we return the accuracy, precision, recall, and F1-score for our trained model on our test data."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you ran into memory issues or want to evaluate a model trained for longer on more data, run the following code block."
      ],
      "metadata": {
        "id": "VK1oRU3pUhMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1n6M1LasX02kEe4KYMiNHbbH0-ZIH1Zio\n",
        "!unzip -o distilbert-for-movie-reviews.zip\n",
        "!rm -rf __MACOSX/"
      ],
      "metadata": {
        "id": "LE_OGLCCUtBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-for-movie-reviews\",\n",
        "                                                           num_labels=2).to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-for-movie-reviews\")"
      ],
      "metadata": {
        "id": "5-fTpX4yV4Ei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following gets your test data and returns metrics for the model."
      ],
      "metadata": {
        "id": "SEmeLlrOV-UB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zRs_sBUAyWZ"
      },
      "outputs": [],
      "source": [
        "test_dataset = getDataset(\"sentiment_data/test\", {\"pos\": 0, \"neg\": 1}, tokenizer, tokenize=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "id": "__WqxyqyAyWZ",
        "tags": []
      },
      "outputs": [],
      "source": [
        "task_evaluator = evaluate.evaluator(\"text-classification\")\n",
        "model.eval()\n",
        "model.to(\"cpu\")\n",
        "results = task_evaluator.compute(\n",
        "    model_or_pipeline=model,\n",
        "    tokenizer=tokenizer,\n",
        "    data=test_dataset,\n",
        "    metric=evaluate.combine([\"accuracy\", \"recall\", \"precision\", \"f1\"]),\n",
        "    label_mapping={\"LABEL_0\": 0.0, \"LABEL_1\": 1.0})\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzFIBSlHAyWZ"
      },
      "source": [
        "## Save Model\n",
        "\n",
        "Finally, we may want to save our final model. We can do that as below. I also show how we can load our pretrained model for use later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eigTUlnfAyWZ"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "trainer.save_model(\"distilbert-for-movie-reviews\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZANYLAX6AyWa"
      },
      "outputs": [],
      "source": [
        "# Load a pretrained model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-for-movie-reviews\",\n",
        "                                                           num_labels=2).to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-for-movie-reviews\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQNh2NB9AyWa"
      },
      "source": [
        "## Reflection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJ4iWa0cTtZ-"
      },
      "source": [
        "1. [25pts] Below, evaluate a non-finetuned version of DistilBERT on our task. Compare the accuracy of that model with your final model. Reflect on your precision, recall, and F1-score. What is your model doing better or worse at?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnvBeIEPT2l8"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1-IhGiHXy7J"
      },
      "source": [
        "2. [10pts] How does the model performance compare to your Naive Bayes' Classifier? What do you think might contribute to the differences between these two models?  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jdKH3m8YXW8"
      },
      "source": [
        "[Write your answer here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d79I5pDzW6QG"
      },
      "source": [
        "3. [5pts] For hyperparameter tuning, what parameters in your model were hyperparameters?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwE5u36rYb1Z"
      },
      "source": [
        "[Write your answer here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rg0nkq2-AyWb"
      },
      "source": [
        "4. [35pts] Run the following code snippet, which evaluates your model on a movie review I wrote. Try out some cases of your own. Does your model work well? Can you come up with cases that trick it?  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tyn3oJYYAyWb"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def getScore(review: str, model: AutoModelForSequenceClassification,\n",
        "            tokenizer: AutoTokenizer) -> str:\n",
        "    \"\"\" Returns the label of the movie review\"\"\"\n",
        "    model.eval()\n",
        "    model = model.to(\"cpu\")\n",
        "    input_ids = tokenizer(review, return_tensors=\"pt\", padding=True,\n",
        "                          truncation=True)\n",
        "    output = model(**input_ids).logits\n",
        "    pred = np.argmax(output, axis=-1).tolist()[0]\n",
        "    if pred == 0:\n",
        "        return \"Positive\"\n",
        "    else:\n",
        "        return \"Negative\"\n",
        "\n",
        "review = \"Sunset Boulevard is an eery movie that deeply upset me. I did love it though.\"\n",
        "getScore(review, model, tokenizer)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "0k3M3VMTAyWX",
        "sohXRw4NAyWY",
        "7MYvT5raAyWZ",
        "uzFIBSlHAyWZ"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}